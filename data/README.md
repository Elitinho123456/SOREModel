# ğŸ“Š Dados da Wikipedia para SOREModel

Este diretÃ³rio contÃ©m dados e utilitÃ¡rios para coletar e processar artigos da Wikipedia em portuguÃªs para treinamento do SOREModel.

## ğŸ“ Arquivos

### `wikipedia_articles_pt.json`
Arquivo principal com **120 links** de artigos da Wikipedia em portuguÃªs, organizados por categorias:

- **CiÃªncia e Tecnologia** (20 artigos): IA, Machine Learning, RobÃ³tica, ProgramaÃ§Ã£o, etc.
- **HistÃ³ria** (20 artigos): Brasil, Portugal, Guerras Mundiais, RevoluÃ§Ãµes, etc.
- **Geografia** (20 artigos): Brasil, Portugal, paÃ­ses da Europa e AmÃ©rica, etc.
- **Literatura** (20 artigos): Autores brasileiros, portugueses e internacionais
- **Artes** (20 artigos): Pintura, MÃºsica, Teatro, Cinema, movimentos artÃ­sticos
- **Esportes** (20 artigos): Futebol, OlimpÃ­adas, esportes diversos
- **PolÃ­tica** (20 artigos): Sistemas polÃ­ticos, organizaÃ§Ãµes internacionais
- **Biologia** (20 artigos): EvoluÃ§Ã£o, genÃ©tica, ecossistemas brasileiros
- **FÃ­sica** (20 artigos): MecÃ¢nica, relatividade, energia, termodinÃ¢mica
- **MatemÃ¡tica** (20 artigos): Ãreas da matemÃ¡tica pura e aplicada

### `wikipedia_utils.py`
UtilitÃ¡rio completo para:
- âœ… Carregar dados do JSON
- âœ… Extrair conteÃºdo de artigos via web scraping
- âœ… Processar mÃºltiplos artigos
- âœ… Salvar conteÃºdo extraÃ­do
- âœ… Gerenciar categorias e URLs

## ğŸš€ Como Usar

### 1. Carregar Dados
```python
from data.wikipedia_utils import WikipediaDataManager

# Inicializar gerenciador
data_manager = WikipediaDataManager()

# Carregar dados
data = data_manager.load_articles_data()

# Ver categorias disponÃ­veis
categories = data_manager.get_categories()
print(f"Categorias: {categories}")

# Pegar URLs de uma categoria
urls = data_manager.get_urls_by_category('ciencia_tecnologia')
print(f"Artigos de ciÃªncia: {len(urls)}")
```

### 2. Extrair ConteÃºdo
```python
# Extrair um artigo especÃ­fico
url = "https://pt.wikipedia.org/wiki/Intelig%C3%AAncia_artificial"
content = data_manager.scrape_article_content(url, max_paragraphs=10)

if content:
    print(f"ConteÃºdo extraÃ­do: {len(content)} caracteres")
    print(content[:200] + "...")
```

### 3. Processar MÃºltiplos Artigos
```python
# Pegar todas as URLs
all_urls = data_manager.get_all_urls()

# Extrair conteÃºdo de vÃ¡rios artigos
results = data_manager.scrape_multiple_articles(
    urls=all_urls[:5],  # Primeiros 5 artigos
    max_paragraphs=20,   # MÃ¡ximo 20 parÃ¡grafos por artigo
    delay=2.0           # 2 segundos entre requests
)

# Salvar resultados
data_manager.save_scraped_content(results, 'data/artigos_processados.json')
```

## ğŸ“‹ Exemplo de Uso com SOREModel

```python
import sys
sys.path.append('src')

from data.wikipedia_utils import WikipediaDataManager
from src import Trainer, SOREModel_v2, criar_tokenizer_basico

# 1. Carregar dados da Wikipedia
data_manager = WikipediaDataManager()
urls = data_manager.get_all_urls()

# 2. Extrair conteÃºdo (exemplo com 10 artigos)
results = data_manager.scrape_multiple_articles(urls[:10], max_paragraphs=15)

# 3. Preparar textos para treinamento
textos_treinamento = [content for url, content in results]

# 4. Treinar modelo
tokenizer = criar_tokenizer_basico()
modelo = SOREModel_v2(
    tamanho_vocab=tokenizer.get_vocab_size(),
    dim_embed=64,
    tamanho_contexto=16,
    num_heads=8,
    num_layers=4
)

trainer = Trainer(modelo, tokenizer)
trainer.treinar(
    textos=textos_treinamento,
    contexto_tamanho=8,
    num_epocas=1000,
    batch_size=4
)
```

## ğŸ“Š EstatÃ­sticas

- **Total de links**: 120
- **Categorias**: 10
- **Artigos por categoria**: 20
- **Idioma**: PortuguÃªs (Brasil)
- **Fontes**: Wikipedia (pt.wikipedia.org)
- **Temas**: Diversos (ciÃªncia, histÃ³ria, geografia, artes, etc.)

## âš ï¸ Avisos Importantes

1. **Respeite os termos de uso** da Wikipedia
2. **Use delays apropriados** entre requests (recomendado: 1-2 segundos)
3. **Limite a quantidade** de artigos processados por vez
4. **Verifique a conectividade** antes de executar scraping em larga escala
5. **Considere usar APIs oficiais** quando disponÃ­veis

## ğŸ”§ FunÃ§Ãµes DisponÃ­veis

### WikipediaDataManager
- `load_articles_data()`: Carrega dados do JSON
- `get_all_urls()`: Retorna todas as URLs
- `get_urls_by_category(category)`: URLs por categoria
- `get_categories()`: Lista de categorias
- `scrape_article_content(url, max_paragraphs)`: Extrai um artigo
- `scrape_multiple_articles(urls, max_paragraphs, delay)`: Extrai mÃºltiplos
- `save_scraped_content(results, output_file)`: Salva resultados

## ğŸ“ˆ ExpansÃ£o Futura

Para expandir a base de dados:

1. **Adicionar mais categorias** no JSON
2. **Incluir mais artigos** por categoria
3. **Diversificar fontes** (alÃ©m da Wikipedia)
4. **Implementar cache** para evitar re-download
5. **Adicionar limpeza de texto** mais sofisticada

## ğŸ¤ ContribuiÃ§Ã£o

Para contribuir com mais links:

1. Adicione URLs no arquivo `wikipedia_articles_pt.json`
2. Mantenha a estrutura por categorias
3. Teste os links antes de submeter
4. Documente novas categorias no README
